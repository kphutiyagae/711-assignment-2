{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:09.704097Z",
     "start_time": "2025-10-01T19:52:09.701791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd #Import pandas for operations\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score,confusion_matrix, ConfusionMatrixDisplay\n"
   ],
   "id": "280d707028e289fa",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.069033Z",
     "start_time": "2025-10-01T19:52:09.724020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#0. Import data\n",
    "dataframe = pd.read_csv('data/sdss_100k_galaxy_form_burst.csv', low_memory=False, header=1)\n",
    "print(\"Dataset loaded successfully!\")"
   ],
   "id": "665bfd057ff64c2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.138253Z",
     "start_time": "2025-10-01T19:52:10.073444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0.1. Data Statistical Analysis\n",
    "print(f\"Dataset shape: {dataframe.shape}\")\n",
    "\n",
    "numerical_features = dataframe.select_dtypes(include='number').columns\n",
    "z_scores = dataframe[numerical_features].apply(zscore)  # compute z-score for each numeric column\n",
    "threshold = 3 #How many STD. we consider outlier.\n",
    "outliers_z = (abs(z_scores) > threshold)\n",
    "\n",
    "print(dataframe[outliers_z.any(axis=1)])  # rows containing at least one outlier\n",
    "# Data is somewhat skewed\n"
   ],
   "id": "a03634ce5ff86a85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 43)\n",
      "                     objid            specobjid          ra        dec  \\\n",
      "4      1237648702973149350   332154249716721664  198.706864  -1.046217   \n",
      "42     1237648721763238209   314301479399745536  170.073523   0.405995   \n",
      "54     1237651192432165212  8192142948407990272  131.241240  53.208364   \n",
      "58     1237651251482067624  7181230230211942400  122.631878  45.281591   \n",
      "72     1237648704061309661  4517153671457560576  231.533375  -0.340179   \n",
      "...                    ...                  ...         ...        ...   \n",
      "99928  1237662236392685639  1384880019989358592  184.933948   9.148592   \n",
      "99936  1237662236929622034  1384887441692846080  185.110612   9.424278   \n",
      "99957  1237654381978845196   623782117049919488  140.589226  53.606884   \n",
      "99968  1237664667902214147  2354413718323881984  165.917906  37.929384   \n",
      "99977  1237664669494804798  1783437871319902208  121.816368  21.689839   \n",
      "\n",
      "                u           g         r         i         z  modelFlux_u  ...  \\\n",
      "4     -9999.00000 -9999.00000  18.37762  18.13383  17.78497     0.000000  ...   \n",
      "42       19.70338    18.40134  17.61436  17.12737  16.91895    13.140040  ...   \n",
      "54       21.50418    21.43728  21.56613  20.34588  22.59260     2.494407  ...   \n",
      "58       22.34181    21.29359  20.37740  19.84789  19.55858     1.139908  ...   \n",
      "72       22.83117    21.50352  20.32544  19.56663  19.26265     0.710520  ...   \n",
      "...           ...         ...       ...       ...       ...          ...  ...   \n",
      "99928    15.99481    14.98151  14.46703  14.23187  14.09406   400.013300  ...   \n",
      "99936    16.09170    14.87027  14.29737  14.00933  13.80514   365.862900  ...   \n",
      "99957    16.31508    15.27625  14.50557  14.06223  13.75321   297.828900  ...   \n",
      "99968    16.24567    15.24830  14.79153  14.43818  14.25075   317.490300  ...   \n",
      "99977    18.81625    17.54841  17.22288  17.02471  16.89137    29.750230  ...   \n",
      "\n",
      "       psfMag_z      expAB_u      expAB_g   expAB_r   expAB_i   expAB_z  \\\n",
      "4      19.02880 -9999.000000 -9999.000000  0.050000  0.050000  0.149973   \n",
      "42     17.77825     0.994886     0.997068  0.963454  0.916301  0.978040   \n",
      "54     22.48610     0.050000     0.368715  0.086463  0.395297  0.054355   \n",
      "58     20.16726     0.050000     0.670824  0.434255  0.388047  0.757089   \n",
      "72     19.88358     0.076835     0.341687  0.529265  0.437189  0.503476   \n",
      "...         ...          ...          ...       ...       ...       ...   \n",
      "99928  18.11176     0.699895     0.830528  0.761708  0.754183  0.733681   \n",
      "99936  17.03037     0.819959     0.936901  0.982233  0.993728  0.986646   \n",
      "99957  16.95848     0.837937     0.824964  0.828468  0.870420  0.837212   \n",
      "99968  16.06469     0.842285     0.832824  0.779422  0.757650  0.769133   \n",
      "99977  19.71383     0.712067     0.793241  0.827162  0.746582  0.765132   \n",
      "\n",
      "        class     subclass  redshift  redshift_err  \n",
      "4      GALAXY  STARFORMING  0.136658      0.000021  \n",
      "42     GALAXY    STARBURST  0.205445      0.000011  \n",
      "54     GALAXY    STARBURST  0.427642      0.000015  \n",
      "58     GALAXY    STARBURST  0.458583      0.000032  \n",
      "72     GALAXY  STARFORMING  0.522535      0.000040  \n",
      "...       ...          ...       ...           ...  \n",
      "99928  GALAXY  STARFORMING  0.019609      0.000013  \n",
      "99936  GALAXY  STARFORMING  0.025180      0.000012  \n",
      "99957  GALAXY  STARFORMING  0.032198      0.000013  \n",
      "99968  GALAXY    STARBURST  0.042913      0.000004  \n",
      "99977  GALAXY  STARFORMING  0.040226      0.000008  \n",
      "\n",
      "[11454 rows x 43 columns]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.274731Z",
     "start_time": "2025-10-01T19:52:10.141476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1 Data pre-processing\n",
    "\n",
    "# 1.1.0 Smoothen out outliers by means of Robust Scaler\n",
    "robust_scaler = RobustScaler() # Initialize scaler\n",
    "robust_scaled_values = robust_scaler.fit_transform(dataframe[numerical_features]) # Scale only numerical data from dataset.\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_values, columns=numerical_features, index=dataframe.index) # Convert scaled data to pandas dataframe NB: index so it aligns with OG.\n",
    "scaled_dataframe = dataframe.copy() # Create a copy of Dataframe to preserve original.\n",
    "scaled_dataframe[numerical_features] = robust_scaled_df # Copy scaled values over into new copy.\n",
    "\n",
    "# 1.1.1 remove unnecessary columns\n",
    "modified_dataframe = scaled_dataframe.drop(['objid', 'specobjid', 'class'], axis=1)"
   ],
   "id": "95a057d80741b5cf",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.286486Z",
     "start_time": "2025-10-01T19:52:10.283333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Begin Training Model.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#3.1. Setup Model with Params.\n",
    "class galaxy_classification_nn(nn.Module):\n",
    "    def __init__(self,  dropout_rate=0.4):\n",
    "\n",
    "        # Store dropout rate.\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = nn.Linear(in_features=39, out_features=133)\n",
    "        self.hidden_layer2 = nn.Linear(in_features=133, out_features=133)\n",
    "        self.output_layer = nn.Linear(in_features=133, out_features=2)\n",
    "\n",
    "        # Specify activation functions.\n",
    "        self.activation_function1 = nn.Tanh() # Input -> Hidden 1\n",
    "        self.activation_function2 = nn.ReLU() # Hidden 1 -> Hidden 2\n",
    "\n",
    "        # Added dropout layers.\n",
    "        self.dropout_layer1 = nn.Dropout(p=self.dropout_rate) #Dropout H1 -> H2\n",
    "        self.dropout_layer2 = nn.Dropout(p=self.dropout_rate) #Droput H2 -> Output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.activation_function1(x)\n",
    "        x = self.dropout_layer1(x)\n",
    "\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.activation_function2(x)\n",
    "        x = self.dropout_layer2(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_weights(self, model_type):\n",
    "        if model_type == 'traditional':\n",
    "            init.xavier_normal_(self.input_layer.weight, )\n",
    "            init.kaiming_normal(self.hidden_layer.weight)\n",
    "            init.xavier_normal_(self.output_layer.weight)\n",
    "            return\n",
    "        elif model_type == 'sign_based':\n",
    "            init.xavier_normal_(self.input_layer.weight)"
   ],
   "id": "634d64608fda4355",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.291946Z",
     "start_time": "2025-10-01T19:52:10.289410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def standardize_data(data, std_type):\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = None\n",
    "\n",
    "    if std_type == 'fit':\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "    else:\n",
    "        data_scaled = scaler.transform(data)\n",
    "\n",
    "    return data_scaled"
   ],
   "id": "8f77019b419019d0",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:52:10.298203Z",
     "start_time": "2025-10-01T19:52:10.294429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#3.2. Init. model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n",
    "\n",
    "galaxy_classification_model = galaxy_classification_nn(dropout_rate=0.4).to(device)\n",
    "\n",
    "#3.2. Setup Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Sign-Based approach setup\n",
    "error_optimizer = optim.Rprop(\n",
    "    galaxy_classification_model.parameters(),\n",
    "    lr=0.01,        # initial step size per weight\n",
    "    etas=(0.5, 1.2),# (eta_minus, eta_plus)\n",
    "    step_sizes=(1e-6, 50.0)  # (min_step, max_step)\n",
    ")\n",
    "\n",
    "#SGD Regular setup - Control\n",
    "control_error_optimizer = optim.SGD(galaxy_classification_model.parameters(), lr=0.01, momentum=0.9)"
   ],
   "id": "e53452af269a2482",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:53:48.768760Z",
     "start_time": "2025-10-01T19:52:11.057887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter optimization: TODO: Move Below\n",
    "# Variables: Initial learning rate and dropout rate\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "learning_rates_list = [0.001, 0.01, 0.05, 0.1]\n",
    "dropout_rates_list = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "results_list = []\n",
    "hpo_epoch_count = 10\n",
    "\n",
    "# 1. Get Dataset Labels & Features TODO: Remove when move down.\n",
    "\n",
    "dataset_features = modified_dataframe.select_dtypes(include=[np.number])\n",
    "dataset_labels = modified_dataframe['subclass'].map({'STARBURST': 1, 'STARFORMING': 0})\n",
    "\n",
    "for learning_rate in learning_rates_list:\n",
    "    for dropout_rate in dropout_rates_list:\n",
    "        print(f\"Learning rate: {learning_rate}, Dropout rate: {dropout_rate}\")\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(k_fold.split(dataset_features, dataset_labels), 1):\n",
    "            print(f\"\\nCurrent Fold {fold}:\")\n",
    "\n",
    "            # 2. Split current folds into test and training set\n",
    "            features_train, features_test = dataset_features.copy().iloc[train_index], dataset_features.copy().iloc[test_index]\n",
    "            labels_train, labels_test =dataset_labels.copy().iloc[train_index], dataset_labels.copy().iloc[test_index]\n",
    "\n",
    "            features_train_numpy = features_train.to_numpy()\n",
    "            features_test_numpy = features_test.to_numpy()\n",
    "\n",
    "            # 3. Normalize dataset\n",
    "            standard_scaler = StandardScaler()\n",
    "            features_train_scaled = standard_scaler.fit_transform(features_train_numpy)\n",
    "            features_test_scaled = standard_scaler.transform(features_test_numpy)\n",
    "\n",
    "            # 4. Convert to Tensors\n",
    "            features_train_tensor = torch.tensor(features_train_scaled, dtype=torch.float32)\n",
    "            features_test_tensor = torch.tensor(features_test_scaled, dtype=torch.float32)\n",
    "            labels_train_tensor = torch.tensor(labels_train.to_numpy(), dtype=torch.long)\n",
    "            labels_test_tensor = torch.tensor(labels_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "            # 5. Create Dataloaders\n",
    "            training_dataset_converted = TensorDataset(features_train_tensor, labels_train_tensor)\n",
    "            testing_dataset_converted = TensorDataset(features_test_tensor, labels_test_tensor)\n",
    "\n",
    "            training_dataset_loader = DataLoader(training_dataset_converted, batch_size=64, shuffle=True)\n",
    "            testing_dataset_loader = DataLoader(testing_dataset_converted, batch_size=64, shuffle=True)\n",
    "\n",
    "            # 6. Compute class weights to address class imbalance\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train.to_numpy())\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "            #Setup current model.\n",
    "            galaxy_classification_model_hpo = galaxy_classification_nn(dropout_rate=dropout_rate).to(device)\n",
    "            loss_function_hpo = nn.CrossEntropyLoss()\n",
    "            error_optimizer_hpo = optim.Rprop(\n",
    "                galaxy_classification_model_hpo.parameters(),\n",
    "                lr=learning_rate,  # initial step size per weight\n",
    "                etas=(0.5, 1.2),# (eta_minus, eta_plus)\n",
    "                step_sizes=(1e-6, 50.0)  # (min_step, max_step)\n",
    "            )\n",
    "\n",
    "            # train for N epochs\n",
    "            for epoch in range(hpo_epoch_count):\n",
    "                galaxy_classification_model_hpo.train()\n",
    "                for feature_batch, label_batch in training_dataset_loader:\n",
    "                    error_optimizer_hpo.zero_grad()\n",
    "                    outputs_hpo = galaxy_classification_model_hpo(feature_batch)\n",
    "                    loss_hpo = loss_function_hpo(outputs_hpo, label_batch)\n",
    "                    loss_hpo.backward()\n",
    "                    error_optimizer_hpo.step()\n",
    "\n",
    "            # evaluate\n",
    "            galaxy_classification_model_hpo.eval()\n",
    "            preds_hpo, true_hpo = [], []\n",
    "            with torch.no_grad():\n",
    "                for feature_batch, label_batch in testing_dataset_loader:\n",
    "                    outputs_eval_hpo = galaxy_classification_model_hpo(feature_batch)\n",
    "                    preds_hpo.extend(torch.argmax(outputs_eval_hpo, dim=1).cpu().numpy())\n",
    "                    true_hpo.extend(label_batch.cpu().numpy())\n",
    "\n",
    "            f1 = f1_score(true_hpo, preds_hpo, average=\"macro\")\n",
    "            fold_scores.append(f1)\n",
    "\n",
    "        avg_f1 = np.mean(fold_scores)\n",
    "        print(f\"Avg F1 for lr={learning_rate}, dropout={dropout_rate}: {avg_f1:.3f}\")\n",
    "        results_list.append({\"lr\": learning_rate, \"dropout\": dropout_rate, \"f1\": avg_f1})\n",
    "\n"
   ],
   "id": "421c3edd7bca9610",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001, Dropout rate: 0.2\n",
      "\n",
      "Current Fold 1:\n",
      "\n",
      "Current Fold 2:\n",
      "\n",
      "Current Fold 3:\n",
      "\n",
      "Current Fold 4:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 71\u001B[39m\n\u001B[32m     69\u001B[39m         loss_hpo = loss_function_hpo(outputs_hpo, label_batch)\n\u001B[32m     70\u001B[39m         loss_hpo.backward()\n\u001B[32m---> \u001B[39m\u001B[32m71\u001B[39m         \u001B[43merror_optimizer_hpo\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[38;5;66;03m# evaluate\u001B[39;00m\n\u001B[32m     74\u001B[39m galaxy_classification_model_hpo.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:516\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    511\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    512\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    513\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    514\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m516\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    519\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     79\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     80\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     83\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\rprop.py:149\u001B[39m, in \u001B[36mRprop.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    143\u001B[39m     maximize = group[\u001B[33m\"\u001B[39m\u001B[33mmaximize\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    145\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    146\u001B[39m         group, params, grads, prevs, step_sizes, state_steps\n\u001B[32m    147\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m     \u001B[43mrprop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    150\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    152\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprevs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    153\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstep_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    155\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstep_size_min\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstep_size_min\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstep_size_max\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstep_size_max\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43metaminus\u001B[49m\u001B[43m=\u001B[49m\u001B[43metaminus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43metaplus\u001B[49m\u001B[43m=\u001B[49m\u001B[43metaplus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:149\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\rprop.py:455\u001B[39m, in \u001B[36mrprop\u001B[39m\u001B[34m(params, grads, prevs, step_sizes, state_steps, foreach, capturable, maximize, differentiable, has_complex, step_size_min, step_size_max, etaminus, etaplus)\u001B[39m\n\u001B[32m    452\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    453\u001B[39m     func = _single_tensor_rprop\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprevs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstep_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstep_size_min\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstep_size_min\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstep_size_max\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstep_size_max\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m    \u001B[49m\u001B[43metaminus\u001B[49m\u001B[43m=\u001B[49m\u001B[43metaminus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[43m    \u001B[49m\u001B[43metaplus\u001B[49m\u001B[43m=\u001B[49m\u001B[43metaplus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documentos\\Academics\\COS711\\Assignment2\\Notebook\\.venv\\Lib\\site-packages\\torch\\optim\\rprop.py:289\u001B[39m, in \u001B[36m_single_tensor_rprop\u001B[39m\u001B[34m(params, grads, prevs, step_sizes, state_steps, step_size_min, step_size_max, etaminus, etaplus, maximize, capturable, differentiable, has_complex)\u001B[39m\n\u001B[32m    286\u001B[39m     grad[sign.eq(etaminus)] = \u001B[32m0\u001B[39m\n\u001B[32m    288\u001B[39m \u001B[38;5;66;03m# update parameters\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m289\u001B[39m param.addcmul_(\u001B[43mgrad\u001B[49m\u001B[43m.\u001B[49m\u001B[43msign\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, step_size, value=-\u001B[32m1\u001B[39m)\n\u001B[32m    290\u001B[39m prev.copy_(grad)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "#\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "accuracy_score_list = []\n",
    "f1_score_list = []\n",
    "epoch_count = 10\n",
    "\n",
    "save_dir = \"saved_classifiers\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "current_best_performing_model = 0.0\n",
    "\n",
    "# 1. Get Dataset Labels & Features\n",
    "dataset_features = modified_dataframe.select_dtypes(include=[np.number])\n",
    "dataset_labels = modified_dataframe['subclass'].map({'STARBURST': 1, 'STARFORMING': 0})\n",
    "\n",
    "print(\"\\nStarting model training:\\n\")\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(k_fold.split(dataset_features, dataset_labels), 1):\n",
    "    print(f\"\\nCurrent Fold {fold}:\")\n",
    "\n",
    "    # 2. Split current folds into test and training set\n",
    "    features_train, features_test = dataset_features.copy().iloc[train_index], dataset_features.copy().iloc[test_index]\n",
    "    labels_train, labels_test =dataset_labels.copy().iloc[train_index], dataset_labels.copy().iloc[test_index]\n",
    "\n",
    "    features_train_numpy = features_train.to_numpy()\n",
    "    features_test_numpy = features_test.to_numpy()\n",
    "\n",
    "    # 3. Normalize dataset\n",
    "    standard_scaler = StandardScaler()\n",
    "    features_train_scaled = standard_scaler.fit_transform(features_train_numpy)\n",
    "    features_test_scaled = standard_scaler.transform(features_test_numpy)\n",
    "\n",
    "    # 4. Convert to Tensors\n",
    "    features_train_tensor = torch.tensor(features_train_scaled, dtype=torch.float32)\n",
    "    features_test_tensor = torch.tensor(features_test_scaled, dtype=torch.float32)\n",
    "    labels_train_tensor = torch.tensor(labels_train.to_numpy(), dtype=torch.long)\n",
    "    labels_test_tensor = torch.tensor(labels_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # 5. Create Dataloaders\n",
    "    training_dataset_converted = TensorDataset(features_train_tensor, labels_train_tensor)\n",
    "    testing_dataset_converted = TensorDataset(features_test_tensor, labels_test_tensor)\n",
    "\n",
    "    training_dataset_loader = DataLoader(training_dataset_converted, batch_size=64, shuffle=True)\n",
    "    testing_dataset_loader = DataLoader(testing_dataset_converted, batch_size=64, shuffle=True)\n",
    "\n",
    "    # 6. Compute class weights to address class imbalance\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train.to_numpy())\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    # 7. Redefine model in training loop.\n",
    "    galaxy_classification_model = galaxy_classification_nn(dropout_rate=0.4).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    error_optimizer = optim.Rprop(\n",
    "        galaxy_classification_model.parameters(),\n",
    "        lr=0.01,        # initial step size per weight\n",
    "        etas=(0.5, 1.2),# (eta_minus, eta_plus)\n",
    "        step_sizes=(1e-6, 50.0)  # (min_step, max_step)\n",
    "    )\n",
    "\n",
    "    # 8. Begin training loop for fold\n",
    "    galaxy_classification_model.train()\n",
    "    for epoch in range(epoch_count):\n",
    "        for features, labels in training_dataset_loader:\n",
    "            error_optimizer.zero_grad()\n",
    "            outputs = galaxy_classification_model(features)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            error_optimizer.step()\n",
    "\n",
    "    # 9. Evaluation loop\n",
    "    galaxy_classification_model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in testing_dataset_loader:\n",
    "            outputs = galaxy_classification_model(features)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "\n",
    "    # Compute metrics\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    accuracy_score_list.append(acc)\n",
    "    f1_score_list.append(f1)\n",
    "\n",
    "    # Per-class precision and recall\n",
    "    precision_per_class = precision_score(all_labels, all_preds, average=None)\n",
    "    recall_per_class    = recall_score(all_labels, all_preds, average=None)\n",
    "\n",
    "    print(f\"Fold {fold} Accuracy: {acc:.3f}, F1: {f1:.3f}\")\n",
    "    for cls_idx, (prec, rec) in enumerate(zip(precision_per_class, recall_per_class)):\n",
    "        print(f\"  Class {cls_idx} -> Precision: {prec:.3f}, Recall: {rec:.3f}\")\n",
    "\n",
    "    print(f\"Fold {fold} Accuracy: {acc:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    # Evaluate if current model is best and save\n",
    "    if f1 > current_best_performing_model:\n",
    "        current_best_performing_model = f1\n",
    "        file_save_path = os.path.join(save_dir, f\"f1_score_{fold}.pt\")\n",
    "        torch.save(galaxy_classification_model.state_dict(), file_save_path)\n",
    "        print(f\"Save current best performing model to {file_save_path} with accuracy {f1:.3f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues', colorbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Average results across folds\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_score_list))\n",
    "print(\"Average F1-score:\", np.mean(f1_score_list))"
   ],
   "id": "b378115b1466a122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T19:26:49.339748Z",
     "start_time": "2025-10-01T19:26:49.337967Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c1428fb0ba4fa0d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f2d9e988a3259ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
