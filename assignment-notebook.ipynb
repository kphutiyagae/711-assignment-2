{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:47.632117Z",
     "start_time": "2025-09-27T23:29:47.630441Z"
    }
   },
   "source": [
    "import pandas as pd #Import pandas for operations\n",
    "from sympy.printing.pytorch import torch"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.063469Z",
     "start_time": "2025-09-27T23:29:47.655670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframe = pd.read_csv('data/sdss_100k_galaxy_form_burst.csv', low_memory=False, header=1)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {dataframe.shape}\")\n",
    "print(f\"Columns: {list(dataframe.columns)}\")\n",
    "print(f\"Datatypes: {dataframe.dtypes}\")"
   ],
   "id": "e8d22af5cfcdf8af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset shape: (100000, 43)\n",
      "Columns: ['objid', 'specobjid', 'ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'modelFlux_u', 'modelFlux_g', 'modelFlux_r', 'modelFlux_i', 'modelFlux_z', 'petroRad_u', 'petroRad_g', 'petroRad_i', 'petroRad_r', 'petroRad_z', 'petroFlux_u', 'petroFlux_g', 'petroFlux_i', 'petroFlux_r', 'petroFlux_z', 'petroR50_u', 'petroR50_g', 'petroR50_i', 'petroR50_r', 'petroR50_z', 'psfMag_u', 'psfMag_r', 'psfMag_g', 'psfMag_i', 'psfMag_z', 'expAB_u', 'expAB_g', 'expAB_r', 'expAB_i', 'expAB_z', 'class', 'subclass', 'redshift', 'redshift_err']\n",
      "Datatypes: objid             int64\n",
      "specobjid        uint64\n",
      "ra              float64\n",
      "dec             float64\n",
      "u               float64\n",
      "g               float64\n",
      "r               float64\n",
      "i               float64\n",
      "z               float64\n",
      "modelFlux_u     float64\n",
      "modelFlux_g     float64\n",
      "modelFlux_r     float64\n",
      "modelFlux_i     float64\n",
      "modelFlux_z     float64\n",
      "petroRad_u      float64\n",
      "petroRad_g      float64\n",
      "petroRad_i      float64\n",
      "petroRad_r      float64\n",
      "petroRad_z      float64\n",
      "petroFlux_u     float64\n",
      "petroFlux_g     float64\n",
      "petroFlux_i     float64\n",
      "petroFlux_r     float64\n",
      "petroFlux_z     float64\n",
      "petroR50_u      float64\n",
      "petroR50_g      float64\n",
      "petroR50_i      float64\n",
      "petroR50_r      float64\n",
      "petroR50_z      float64\n",
      "psfMag_u        float64\n",
      "psfMag_r        float64\n",
      "psfMag_g        float64\n",
      "psfMag_i        float64\n",
      "psfMag_z        float64\n",
      "expAB_u         float64\n",
      "expAB_g         float64\n",
      "expAB_r         float64\n",
      "expAB_i         float64\n",
      "expAB_z         float64\n",
      "class            object\n",
      "subclass         object\n",
      "redshift        float64\n",
      "redshift_err    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.067966Z",
     "start_time": "2025-09-27T23:29:48.066305Z"
    }
   },
   "cell_type": "code",
   "source": "# dataframe['subclass'].unique()\n",
   "id": "3fdbe24f7d64288f",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.114320Z",
     "start_time": "2025-09-27T23:29:48.072627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#1 Data pre-processing\n",
    "\n",
    "#1.1.1 remove unnecessary columns\n",
    "modified_dataframe = dataframe.drop(['objid', 'specobjid', 'class'], axis=1)\n",
    "\n",
    "# #1.1.2 Encode subclass category accordingly\n",
    "# modified_dataframe['subclass'] = modified_dataframe['subclass'].map({'STARBURST': 1, 'STARFORMING': 0})\n",
    "\n",
    "#1.2.1 Split dataset into Test and Training data. (80% Training - 20% Test)\n",
    "main_training_data = modified_dataframe.sample(frac=0.8)\n",
    "testing_data = modified_dataframe.drop(main_training_data.index)\n",
    "\n",
    "#1.2.2 Split Test data into Test and Validation Set. (70% Training - 30% Validation)\n",
    "training_set = main_training_data.sample(frac=0.7)\n",
    "validation_set = main_training_data.drop(training_set.index)\n"
   ],
   "id": "cf39c68afc7d1fe0",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.120502Z",
     "start_time": "2025-09-27T23:29:48.118207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Visualize data.\n",
    "#1. Training Set Info\n",
    "print(f\"Training Dataset shape: {training_set.shape}\")\n",
    "# print(f\"Training Dataset Columns: {list(training_set.columns)}\")\n",
    "\n",
    "#2. Testing Set Info\n",
    "print(f\"Testing Dataset shape: {testing_data.shape}\")\n",
    "# print(f\"Testing Dataset Columns: {list(testing_data.columns)}\")\n",
    "\n",
    "#3. Validation Set Info\n",
    "print(f\"Validation Dataset shape: {validation_set.shape}\")\n",
    "# print(f\"Validation Dataset Columns: {list(validation_set.columns)}\")\n",
    "\n",
    "assert(len(training_set) + len(testing_data) + len(validation_set) == len(modified_dataframe)) #Confirm that the lengths match.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "241d8fbdad094eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset shape: (56000, 40)\n",
      "Testing Dataset shape: (20000, 40)\n",
      "Validation Dataset shape: (24000, 40)\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.128520Z",
     "start_time": "2025-09-27T23:29:48.124942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distributions_seaborn(df, figsize=(15, 12)):\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Plot numerical distributions\n",
    "    if len(numerical_cols) > 0:\n",
    "        n_rows = (len(numerical_cols) + 2) // 3\n",
    "        fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, col in enumerate(numerical_cols):\n",
    "            sns.histplot(data=df, x=col, ax=axes[i], kde=True, bins=30)\n",
    "            axes[i].axvline(df[col].mean(), color='red', linestyle='--', alpha=0.8)\n",
    "            axes[i].axvline(df[col].median(), color='green', linestyle='--', alpha=0.8)\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "\n",
    "        # Hide empty subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot categorical distributions\n",
    "    if len(categorical_cols) > 0:\n",
    "        n_rows = (len(categorical_cols) + 2) // 3\n",
    "        fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            value_counts = df[col].value_counts().head(10)\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].set_ylabel('Count')\n",
    "\n",
    "        # Hide empty subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# plot_distributions_seaborn(training_set)"
   ],
   "id": "9747cec447fa3143",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.225386Z",
     "start_time": "2025-09-27T23:29:48.140519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#2. Normalize data.\n",
    "\n",
    "#2. Apply mean-centering & variance scaling to data\n",
    "\n",
    "#2.1. Define function\n",
    "def standardize_with_sklearn(df):\n",
    "    \"\"\"\n",
    "    Use scikit-learn's StandardScaler for robust standardization\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df['subclass']\n",
    "\n",
    "    # Create a copy and only scale numerical columns\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    if len(numerical_cols) > 0:\n",
    "        scaler = StandardScaler()\n",
    "        df_normalized[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "        df_normalized.drop('subclass', axis=1, inplace=True)\n",
    "\n",
    "    return df_normalized, categorical_cols\n",
    "\n",
    "#2.2. Apply standardization\n",
    "testing_dataset_normalized, testing_dataset_labels = standardize_with_sklearn(testing_data)\n",
    "training_dataset_normalized, training_dataset_labels = standardize_with_sklearn(training_set)\n",
    "validation_dataset_normalized, validation_dataset_labels = standardize_with_sklearn(validation_set)\n",
    "\n",
    "# testing_dataset_normalized.head(5)\n",
    "# testing_dataset_normalized.shape\n",
    "# testing_dataset_labels.shape\n",
    "# validation_dataset_normalized.shape\n",
    "# validation_dataset_labels.shape\n",
    "#1.1.2 Encode subclass category accordingly\n",
    "testing_dataset_labels = testing_dataset_labels.map({'STARBURST': 1, 'STARFORMING': 0})\n",
    "training_dataset_labels = training_dataset_labels.map({'STARBURST': 1, 'STARFORMING': 0})\n",
    "validation_dataset_labels = validation_dataset_labels.map({'STARBURST': 1, 'STARFORMING': 0})\n"
   ],
   "id": "28b46100eb6db7c4",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.231159Z",
     "start_time": "2025-09-27T23:29:48.228494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "#3. Begin Training Model.\n",
    "# print(torch.cuda.is_available())\n",
    "\n",
    "#3.1. Setup Model with Params. TODO: Specify weights!!\n",
    "class galaxy_classification_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_features=39, out_features=133)\n",
    "        self.hidden_layer = nn.Linear(in_features=133, out_features=133)\n",
    "        self.output_layer = nn.Linear(in_features=133, out_features=2)\n",
    "\n",
    "        #Specify activation functions.\n",
    "        self.activation_function1 = nn.Tanh() # Input -> Hidden 1\n",
    "        self.activation_function2 = nn.ReLU() # Hidden 1 -> Hidden 2\n",
    "        self.activation_function3 = nn.Softmax(dim=1) # Hidden 2 -> Output TODO: Validate dim param\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation_function1(x)\n",
    "\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.activation_function2(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        x = self.activation_function3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        # x = F.relu(self.fc1(x)) # TODO: Check why not linear.\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        # x = self.fc3(x)                  # raw scores (logits)\n",
    "        return x\n"
   ],
   "id": "943943929ce0e60c",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.245356Z",
     "start_time": "2025-09-27T23:29:48.234494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "training_features = training_dataset_normalized.values\n",
    "\n",
    "training_labels = training_dataset_labels.values\n",
    "\n",
    "testing_features = testing_dataset_normalized.values\n",
    "\n",
    "testing_labels = testing_dataset_labels.values\n",
    "\n",
    "\n",
    "# TODO: Add Validation tensors\n",
    "training_tensor_features = torch.tensor(training_features, dtype=torch.float32)\n",
    "\n",
    "training_tensor_labels = torch.tensor(training_labels, dtype=torch.float32)\n",
    "\n",
    "testing_tensor_features = torch.tensor(testing_features, dtype=torch.float32)\n",
    "\n",
    "testing_tensor_labels = torch.tensor(testing_labels, dtype=torch.float32)\n",
    "\n",
    "# Create Datasets\n",
    "training_dataset = TensorDataset(training_tensor_features, training_tensor_labels)\n",
    "testing_dataset = TensorDataset(testing_tensor_features, testing_tensor_labels)\n",
    "\n",
    "training_dataset_loader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "testing_dataset_loader = DataLoader(testing_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(training_tensor_features.dtype)\n",
    "# print(training_dataset_labels)\n",
    "\n",
    "# training_dataset_normalized.head(5)\n",
    "#\n",
    "# train_tensor_features = torch.tensor(training_dataset_normalized.values, dtype=torch.float32)\n",
    "#\n",
    "# train_tensor_labels = torch.tensor(training_dataset_labels.values, dtype=torch.float32)\n",
    "#\n",
    "# training_dataset = TensorDataset(train_tensor_features, train_tensor_labels)\n",
    "#"
   ],
   "id": "3ea0e49f54ee9a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:29:48.272608Z",
     "start_time": "2025-09-27T23:29:48.269777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#3.2. Init. model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n",
    "\n",
    "galaxy_classification_model = galaxy_classification_nn().to(device)\n",
    "\n",
    "#3.2. Setup Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "error_optimizer = optim.SGD(galaxy_classification_model.parameters(), lr=0.01)\n"
   ],
   "id": "14e996b8ea7bd1f2",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:32:19.403958Z",
     "start_time": "2025-09-27T23:32:17.188623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(5):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    # galaxy_classification_model.train()\n",
    "    for  features, labels in training_dataset_loader:\n",
    "        # print(f\"Features: {features.shape}\")\n",
    "        # print(f\"Targets: {targets.shape}\")\n",
    "        # print(f\"Features DataType: {type(features)}\")\n",
    "        # print(f\"Labels DataType: {type(targets)}\")\n",
    "        # print(features.shape, labels.shape)\n",
    "        # print(labels)\n",
    "\n",
    "        outputs = galaxy_classification_model(features)\n",
    "        # print(f\"Model outputs shape: {outputs.shape}\")\n",
    "    #     batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "    #\n",
    "    #\n",
    "    # error_optimizer.zero_grad()\n",
    "    #\n",
    "    # logits = galaxy_classification_model(batch_X)\n",
    "    # loss = loss_function(logits, batch_y)\n",
    "    # loss.backward()\n",
    "    # error_optimizer.step()\n",
    "    # print(f\"Epoch {epoch+1} completed\")\n",
    "\n"
   ],
   "id": "5da70087e978e354",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n"
     ]
    }
   ],
   "execution_count": 123
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
